
SOLITA RETE - DIVERSI componenti della GMM con 400 samples:
2023-10-26 19:00:06	ec913e5	4	MLP kmeans	MULTIVARIATE_1254	400	1	0.68	0.03	0.14	0.68	0.01	0.04	0440b	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	1609d
2023-10-26 19:00:30	937738f	8	MLP kmeans	MULTIVARIATE_1254	400	1	0.84	0.02	0.1	0.91	0.01	0.01	eefa1	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	1609d
2023-10-26 19:01:04	b125ee7	16	MLP kmeans	MULTIVARIATE_1254	400	1	0.9	    0.02	0.09	0.9	0	0.02	fde75	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	1609d
2023-10-26 19:01:23	40ea956	32	MLP kmeans	MULTIVARIATE_1254	400	1	0.95	0.01	0.06	0.95	0	0.01	Infinity	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	1609d
2023-10-26 19:06:25	f844fcc	64	MLP kmeans	MULTIVARIATE_1254	400	1	0.92	0.02	0.06	0.94	0	0.01	02d9d	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	1609d
--> incremento della R2 all'aumentare dei componenti fino a 64 dove inizia a riscendere

SOLITA RETE - DIVERSI componenti della GMM con 200 samples:
2023-10-26 19:12:18	a2b809c	4	MLP kmeans	MULTIVARIATE_1254	200	1	0.43	0.04	0.12	0.56	0.02	0.07	79c8b	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	37138
2023-10-26 19:12:37	2707790	8	MLP kmeans	MULTIVARIATE_1254	200	1	0.55	0.04	0.09	0.75	0.01	0.05	95ba9	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	37138
2023-10-26 19:12:52	cae3645	16	MLP kmeans	MULTIVARIATE_1254	200	1	0.62	0.04	0.1	0.74	0.01	0.04	5bb64	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	37138
2023-10-26 19:13:07	6d0f0c4	32	MLP kmeans	MULTIVARIATE_1254	200	1	0.12	0.05	0.1	0.46	0.03	0.1	d463c	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	37138
2023-10-26 19:14:19	32e3ee8	64	MLP kmeans	MULTIVARIATE_1254	200	1	-0.04	0.06	0.11	0.36	0.03	0.1	1d85e	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	37138
--> qui lo swett spot è intorno a 16 componenti

SOLITA RETE - DIVERSI componenti della GMM con 100 samples:
696c7f9	4	MLP kmeans	MULTIVARIATE_1254	100	1	0.44	0.04	0.07	0.56	0.02	0.08	81cf3	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	e83ad
71d8b64	8	MLP kmeans	MULTIVARIATE_1254	100	1	-0.01	0.06	0.1	0.43	0.03	0.11	53cc2	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	e83ad
98061a4	16	MLP kmeans	MULTIVARIATE_1254	100	1	-0.54	0.07	0.11	0.49	0.05	0.1	147cd	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	e83ad
d5c3783	32	MLP kmeans	MULTIVARIATE_1254	100	1	-0.52	0.07	0.13	0.2	0.05	0.15	afeee	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	e83ad
4d6b12b	64	MLP kmeans	MULTIVARIATE_1254	100	1	-0.28	0.06	0.17	-0.21	0.04	0.28	df9dd	{'criterion': <class 'torch.nn.modules.loss.HuberLoss'>, 'max_epochs': 100, 'batch_size': 4, 'lr': 0.001, 'module__last_activation': 'lambda', 'module__hidden_layer': [(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dropout': 0.2}	{'criterion': [<class 'torch.nn.modules.loss.HuberLoss'>], 'max_epochs': [100], 'batch_size': [4, 16, 32], 'lr': [0.001, 0.003], 'module__last_activation': ['lambda', None], 'module__hidden_layer': [[(16, ReLU()), (32, LeakyReLU(negative_slope=0.2)), (32, LeakyReLU(negative_slope=0.2)), (16, ReLU())], [(32, LeakyReLU(negative_slope=0.01)), (32, Tanh()), (64, ReLU())], [(8, LeakyReLU(negative_slope=0.01)), (8, Tanh())]], 'optimizer': [<class 'torch.optim.adam.Adam'>], 'module__dropout': [0.2]}	100	[[{'type': 'exponential', 'rate': 1, 'weight': 0.2}, {'type': 'logistic', 'mean': 4, 'scale': 0.8, 'weight': 0.25}, {'type': 'logistic', 'mean': 5.5, 'scale': 0.7, 'weight': 0.3}, {'type': 'exponential', 'mean': -1, 'weight': 0.25, 'shift': -10}]]	e83ad
--> in questo caso con pochi componenti ottengo risultati migliori


